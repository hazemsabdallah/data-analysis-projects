{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrangle and Analyze Data @WeRateDogs\n",
    "***\n",
    "### Gathering\n",
    "\n",
    "For the first part of the wrangling process, three `pd.DataFrames` were gathered from different sources. The first being `twitter_archive_enhanced` which is given in CSV format and reading it in is pretty straightforward. The second being `image_predictions` which is downloaded programmatically using, the requests library, into a TSV format file and again reading it in is pretty similar to the former except for identifying the type of separator. The third source is collected via twitter API with the help of Python’s tweepy access library. This is done by looping through all the archived tweets and getting their JSON data and downloading them into a TXT format file. These tweet JSON data are later used to create a third table; namely, `twitter_api`.\n",
    "\n",
    "### Assessing\n",
    "\n",
    "For the second and third parts of the wrangling process, several issues in data quality and tidiness are assessed and cleaned. Starting by viewing the dataset in a spreadsheet and text editor programs to visually assess and spot obvious messy or dirty data issues. The most noticeable issues include variables spread over several columns, ‘None’ values instead of ‘np.nan’, dataset containing both original tweets as well as retweets (which means duplicated ratings is an issue). In addition, this visual assessment resulted in a column glossary to be used as a reference for better understanding of each data field. Furthermore, programmatic assessment helped with spotting several more issues. This is accomplished using methods such as DataFrame slicing, `info()`, `value_counts()`, `isnull()`, `isin()`, `nunique()`, …etc. More issues such as invalid dog names, invalid datatypes, and inconsistent ratings are spotted. A summary of all data quality and tidiness issues is included right before the cleaning part for easy navigation.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "For the cleaning part, whenever possible, I attempted fixing the tidiness issues first just to make it easier with fixing quality issues later and not go back and forth. However, this is not always the case because some tidiness issues are dependent on quality issues which needed to be addressed first. At any rate, all cleaning operations are performed on copied `pd.DataFrames` so that original tables could be easily accessed. Whereas some cleaning operations are as straightforward as casting a datatype or replacing a value using `to_datetime`, `astype()` or `replace()` methods, others are more iterative using `for` loops and created lists and dictionaries. \n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
